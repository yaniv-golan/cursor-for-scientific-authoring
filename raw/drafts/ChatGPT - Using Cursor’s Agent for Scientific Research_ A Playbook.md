# Using Cursor’s Agent for Scientific Research: A Playbook

## Introduction

Cursor 2.0 repositioned the product from a file-centric IDE into an agent-first research workspace anchored by Composer, Cursor’s own low-latency model for autonomous coding and analysis.[[38]](https://cursor.com/changelog/2-0)[[39]](https://cursor.com/blog/2-0) You still get the familiar editor, but the flagship experience now revolves around orchestrating agents that plan, execute, and verify work on your behalf. In this playbook, we’ll guide you through harnessing those agentic capabilities for scientific research—from capturing project norms in AGENTS.md, to delegating literature reviews, to compiling a well-cited report in Markdown. The objective is to help non-coders tap Cursor’s multi-step agents, built-in Composer model, and tight filesystem integration to accelerate research workflows.

## Understanding Cursor’s Agentic Capabilities

Cursor 2.0 introduced a redesigned interface that **centers around AI agents** rather than just files. With the 2.0 release, Cursor added Composer—its first proprietary agentic coding model—and a workspace that treats multi-agent orchestration as a first-class capability[\[38\]](https://cursor.com/changelog/2-0)[\[39\]](https://cursor.com/blog/2-0). You can now run multiple AI agents in parallel without them stepping on each other’s changes. These agents are not limited to completing code; they can follow complex instructions, use built-in tools like a browser, and iterate on tasks autonomously. In practice, an agent can be instructed to perform research steps (like searching for literature, reading files, writing summaries) in a way that resembles a junior researcher following a plan.

* **Multi-Agent and Parallel Workflows:** Cursor 2.0 supports up to eight concurrent agents, isolated via git worktrees or remote sandboxes so they can execute different research subtasks safely[\[38\]](https://cursor.com/changelog/2-0). You might assign one agent to gather papers while another synthesizes findings, then reconcile their outputs once both finish.  
* **Composer Model (Cursor’s Own):** Composer acts as Cursor’s first-party coding-and-research model, marketed as four times faster than similarly capable models and tuned for agentic execution[\[39\]](https://cursor.com/blog/2-0). It was trained with codebase-wide semantic search so it excels at navigating large repositories; for non-coding or highly specialized research tasks, experiment with Claude or GPT variants to see which reasoning style fits best.  
* **Integration with AI Models (OpenAI & Anthropic):** Cursor isn’t tied to a single AI provider. It can call OpenAI’s GPT-4/5 family—including the GPT-5-Codex model—alongside Anthropic’s current Claude lineup (Haiku 4.5, Sonnet 4.5, Opus 4.1) once you add the relevant keys[\[2\]](https://davidmelamed.com/2025/08/08/overview-of-advanced-ai-coding-agents-august-2025/#:~:text=,%E2%80%93%20you%20can%20converse%20with)[\[3\]](https://davidmelamed.com/2025/08/08/overview-of-advanced-ai-coding-agents-august-2025/#:~:text=,world%20coding%20tasks)[\[40\]](https://docs.anthropic.com/claude/docs/models-overview). OpenAI Codex itself is a separate agentic product (CLI, IDE, cloud) that uses those APIs; in Cursor you’re invoking the models directly rather than the Codex app[[37]](https://openai.com/codex/). Likewise, Anthropic’s **Claude Code** is its own terminal/VS Code assistant, but the same Claude models can serve as Cursor backends when configured via API settings[\[41\]](https://docs.anthropic.com/claude/docs/claude-code-overview). Choose the mix that fits each agent: GPT-4/5 for broad knowledge, Claude Sonnet for long-context synthesis, or Composer when you want lowest latency.  
* **“Thinking” Mode:** When tackling complex problems, you can enable a more **deliberative reasoning mode** (often called *Thinking mode* in Cursor’s interface for Claude). In normal mode, the agent gives quick answers, but in thinking mode it will take extra steps to reason through the task. One user found that enabling Claude’s thinking mode yielded a more thorough draft for an academic summary (at the cost of some increased risk of AI “creativity” or hallucinations)[\[4\]](https://forum.cursor.com/t/anyone-use-cursor-for-academic-research-writing/57393#:~:text=I%20get%20the%20sense%20I%E2%80%99ll,experiment%20I%E2%80%99ll%20update%20with%20more). Use this mode for challenging research questions, but always review the output for accuracy.  
* **Built-in Tools (Browser & Terminal):** Cursor’s browser graduated from beta in the 2.0 release and now ships inside the editor with DOM inspection tools, so agents can capture evidence while browsing or testing UI flows[\[38\]](https://cursor.com/changelog/2-0). Terminals invoked by agents on macOS default to a sandboxed environment, reducing the risk of unreviewed commands; you can still open an unsandboxed shell when you explicitly need one[\[38\]](https://cursor.com/changelog/2-0). For scientific research, that means agents can search the web or execute analyses safely as part of their plans.  
* **Voice Mode:** Prefer speaking instructions? Cursor’s voice mode lets you steer agents hands-free—a convenient option when you’re annotating papers or running lab instruments away from the keyboard.[\[38\]](https://cursor.com/changelog/2-0)  
* **Plan Mode & Team Commands:** Plan Mode lets one model draft and another execute multi-step plans (even in the background), while Team Commands push shared rules to every teammate so you no longer maintain local `.cursorrules` files.[\[38\]](https://cursor.com/changelog/2-0)[\[44\]](https://cursor.com/docs/team/commands)  
* **Self-Gathered Project Context:** Cursor stripped out many manual @Definitions/@Link panels in 2.0 and instead lets agents auto-discover relevant files. You can still cite files explicitly with `@`, but expect the agent to collect context from the workspace on its own[\[38\]](https://cursor.com/changelog/2-0).  
* **Cloud Agent Reliability:** Cursor now advertises 99.9 % reliability and near-instant startup for cloud agents[\[38\]](https://cursor.com/changelog/2-0). That makes remote execution viable for long-running scientific workflows where you might previously have kept everything local.

## Setting Up Your Research Environment in Cursor

Before diving into queries, it’s important to configure your project so the AI knows the context and guidelines for your research task. This is where **AGENTS.md** comes in.

### Creating an AGENTS.md for Your Project

**AGENTS.md** is a special markdown file placed at the root of your project that provides instructions and context to AI agents[\[6\]](https://agents.md/#:~:text=README,project%20descriptions%2C%20and%20contribution%20guidelines). Think of it as a “README for the AI”. While your regular README.md is for human collaborators, AGENTS.md is meant for the AI – containing any information that would help it work effectively on your project. Cursor (and many other AI dev tools) will automatically load AGENTS.md content into the agent’s context on each interaction, so the agent always “remembers” these rules or notes.

> **Note for Claude users:** Claude Code does not automatically read `AGENTS.md`. Create a `CLAUDE.md` file in the project and include `@AGENTS.md` inside it to keep a single source of truth that both Cursor and Claude share.[\[43\]](https://docs.claude.com/en/docs/claude-code/claude-code-on-the-web#best-practices)

In the context of scientific research, here’s how you can utilize AGENTS.md:

* **Outline the Research Objective:** Provide a short overview of what topic or question the research is addressing. For example: “**Project Overview:** Investigating the effects of climate change on coral reef biodiversity.” This gives the AI a high-level understanding of the goal.  
* **Specify Output Requirements:** Clearly state what you expect the AI to produce. Since we want a well-cited report, you might add a section like **“Report Guidelines”** stating: “The agent should compile findings into report.md in Markdown format, with an Introduction, Methods, Findings, and Conclusion.” By specifying the file name (report.md), you hint that the agent should write to that file (Cursor’s agent can create or modify files when instructed).  
* **Citation Format and Source Expectations:** It’s critical to enforce good citation practices here. In AGENTS.md, include rules about citations. For example:

\#\# Citation and Evidence Rules  
\- \*\*Cite all sources\*\* for factual claims. Every important claim should reference \*\*at least 2 reputable scientific sources\*\* (e.g. peer-reviewed journals, official reports).  
\- \*\*Citation format:\*\* Use numeric bracketed citations (e.g. \[1\], \[2\]) or footnotes as needed, and include a bibliography section with the full references.  
\- Prefer references that are recent (last 5-10 years) unless historical context is needed.  
\- Do \*\*not\*\* cite unreliable sources (no personal blogs, only use Wikipedia for leads but not as a cited source).

These instructions will guide the AI’s behavior. By setting the expectation of multiple sources per claim, you reduce the chance of the agent making unsupported statements. The agent will see these rules each time, increasing the likelihood it follows them.

* **Style and Tone Guidelines:** If needed, note the desired tone (e.g. formal academic tone, or simplified language if for a general audience) and any specific formatting (like “use APA style for references” or “use Markdown headings for each section”). For example, you could add: “Write in a neutral, scientific tone. Use clear, concise paragraphs. Avoid first-person; this should read like a research report.” These instructions help the AI tailor the output to the expected style.  
* **Agent Behavior Preferences:** You can also instruct *how* you want the agent to approach tasks. For instance: “When gathering information, always **quote or paraphrase accurately** from sources and then cite them. Do not fabricate data or quotes.” Or, “If the agent is unsure of a detail, it should search for an answer or ask for clarification rather than guessing.” Such rules make your expectations explicit. Essentially, you are configuring how the AI “thinks” and behaves inside this project. In Cursor’s terminology, these are like the old “.cursorrules” – a way to define the AI’s behavior within the project[\[7\]](https://www.reddit.com/r/ClaudeAI/comments/1oj109n/claude_skills_are_just_cursorrules_change_my_mind/#:~:text=The%20essence%20is%20that%20with,functional%20capability). (As a point of reference, *Cursor rules (now AGENTS.md) define how the AI should behave or think in a project context, whereas skills or tools define what actions it can take – in other words,* *behavioral context vs. functional capability*\*[\[7\]](https://www.reddit.com/r/ClaudeAI/comments/1oj109n/claude_skills_are_just_cursorrules_change_my_mind/#:~:text=The%20essence%20is%20that%20with,functional%20capability).)

Once you’ve created an AGENTS.md with sections covering the above, save it at the project root. Cursor will detect it. (Most AI agent tools look for this file automatically; it’s an emerging standard used by 20k+ projects[\[8\]](https://agents.md/#:~:text=A%20simple%2C%20open%20format%20for,source%20projects).) The next time you prompt the agent, it will read these instructions. This means even if you just ask, “Give me an outline of the report,” the agent knows (from AGENTS.md) that it must eventually cite sources and meet certain standards.

**Tip:** You can ask Cursor to help you draft the AGENTS.md itself\! If you tell the agent, “Generate an AGENTS.md template for this project,” it might create a scaffold with headings (Project Overview, Instructions, etc.) based on best practices[\[9\]](https://agents.md/#:~:text=Add%20sections%20that%20help%20an,Popular%20choices). You can then fill in the specifics. Treat AGENTS.md as living documentation – update it as you refine your requirements[\[10\]](https://agents.md/#:~:text=What%20if%20instructions%20conflict%3F)[\[11\]](https://agents.md/#:~:text=Can%20I%20update%20it%20later%3F).

### Choosing Your AI Model & Enabling Web Access

After AGENTS.md is set, decide whether Composer (the default Cursor model) is sufficient or if you need supplementary APIs. Composer now powers most built-in agent flows; add vendor keys only when you want OpenAI or Anthropic models for comparison or specialized behavior. In Cursor’s settings or when you open a new agent, you can pick Composer or specific GPT-4/5 and Claude models—just link the relevant API keys before selecting them. If you install Anthropic’s Claude Code VS Code extension inside Cursor, it appears as a separate side panel you can invoke alongside (but distinct from) Cursor’s own agents.

For research tasks that require reading a lot of content or keeping track of many sources, Claude Sonnet 4.5 or Opus 4.1 are strong API backends because of their large context windows and access to Claude Skills. If you need the latest knowledge and web access, make sure the agent or extension you launch can perform browsing. Cursor’s built-in agents can call MCP tools when configured, while the Claude Code extension ships with its own browser and skills toolkit.

As of 2025, Cursor introduced a *native browser integration* which agents can use to test code or fetch information[\[5\]](https://www.deeplearning.ai/the-batch/cursor-introduces-a-new-model-built-for-agents/#:~:text=projects,limited%20features%20and%20paid%20plans). Check Cursor’s documentation on how to invoke tool use. Often, you might simply instruct the agent within your prompt to search the web or retrieve a URL (the agent may then use the browser tool under the hood). For example: “Search for recent NASA reports on coral bleaching and summarize key findings (cite the reports).” A well-configured agent will interpret that it should perform a web search, open relevant pages, and extract information.

If the agent does not automatically use the browser when you expect, you can try a direct approach: e.g., “Use the browser to find information on X.” In some cases, you might have to copy in important information manually (for instance, pasting an excerpt from an article into a file and referencing it) if the agent lacks direct search capability. We’ll cover that next.

### Installing the OpenAI Codex CLI (optional)

If you rely on OpenAI Codex alongside Cursor, installing the Codex CLI ensures the same automation workflows available through Codex’s IDE extension, cloud workspace, GitHub PR hooks, and iOS app are accessible from your terminal.[\[37\]](https://openai.com/codex/) The CLI is distributed as a Homebrew cask, so run `brew install --cask codex` for the initial install and `brew upgrade --cask codex` to stay current.[\[35\]](https://formulae.brew.sh/cask/codex?utm_source=chatgpt.com)[\[36\]](https://developers.openai.com/codex/cli/?utm_source=chatgpt.com) Homebrew now recommends the cask, while the legacy `brew install codex` formula remains available in a deprecated state—using the cask avoids the intermittent install failures some users reported on macOS.

Remember that Codex is a standalone OpenAI product, not a Cursor feature. It ships as a multi-surface coding agent with a cloud IDE (complete with managed sandboxes), CLI, Slack and GitHub integrations, and an optional VS Code/Cursor/Windsurf extension that you can enable when you want the Codex UI inside your editor.[\[45\]](https://developers.openai.com/codex/cloud)[\[46\]](https://developers.openai.com/codex/ide) Codex jobs run on GPT-5-Codex, require a paid ChatGPT plan (Plus, Pro, Business, Edu, or Enterprise), and can execute tasks in parallel in the cloud while streaming diffs back to your repo.[\[37\]](https://openai.com/codex/)[\[45\]](https://developers.openai.com/codex/cloud) The Codex Agents SDK (TypeScript) and MCP support let you build custom toolchains that hand off work to Codex with the same `AGENTS.md` configuration style Cursor uses.[\[47\]](https://developers.openai.com/codex/sdk)[\[48\]](https://github.com/openai/codex/blob/main/docs/config.md#mcp_servers) When running Codex alongside Cursor, treat them as two cooperating agents: launch Codex in its own panel or terminal, point it at your project, and keep `AGENTS.md` authoritative so both assistants stay aligned.

## Feeding the Agent with Sources and Data

A research assistant is only as good as its sources. There are a few ways to give Cursor’s agent access to high-quality information:

### 1\. **Providing Reference Files in the Workspace**

One straightforward method is to supply the AI with the text of sources you consider important. For example, if you have several scientific papers (PDFs), you can convert them to text or Markdown and add them to your project (perhaps in a references/ folder). Name them clearly (e.g., Smith2023\_climate\_coral.txt).

Now, using Cursor’s **@ reference** feature, you can inject these files into the AI’s context. In a prompt or instruction, you might write: *“Refer to the findings in @references/Smith2023\_climate\_coral.txt and @references/Jones2021\_reef\_loss.md when discussing coral bleaching rates.”* Cursor will include the content of those files for the AI to draw from[\[12\]](https://forum.cursor.com/t/cursor-prompt-engineering-best-practices/1592#:~:text=success%20with%20cursor%20is%20all,about%20context%20management)[\[13\]](https://forum.cursor.com/t/cursor-prompt-engineering-best-practices/1592#:~:text=also%20,cursorrules). This reduces hallucination and increases factual accuracy, because the model can quote or paraphrase actual text from the sources you provided.

A user on the Cursor forum described a successful setup for academic writing using this approach: they had a markdown file for the section they were writing, a directory of two papers’ text, and **two rule files** (one containing the funding agency’s requirements and one with academic writing guidelines)[\[14\]](https://forum.cursor.com/t/anyone-use-cursor-for-academic-research-writing/57393#:~:text=Sat%20down%20and%20did%20a,sonnet%20%28w%2Fo%20thinking). In agent mode, they gave the AI these references as context, and the AI was able to draft a project summary using the real content from those papers[\[14\]](https://forum.cursor.com/t/anyone-use-cursor-for-academic-research-writing/57393#:~:text=Sat%20down%20and%20did%20a,sonnet%20%28w%2Fo%20thinking). The draft improved when they explicitly reminded the agent about the writing style rules (using an @ reference to the writing rules file) and when they enabled the thinking mode for deeper analysis[\[15\]](https://forum.cursor.com/t/anyone-use-cursor-for-academic-research-writing/57393#:~:text=I%20added%20the%20writing%20rule,improved%20the%20draft%20a%20bit)[\[4\]](https://forum.cursor.com/t/anyone-use-cursor-for-academic-research-writing/57393#:~:text=I%20get%20the%20sense%20I%E2%80%99ll,experiment%20I%E2%80%99ll%20update%20with%20more). This anecdote illustrates the power of combining **provided references \+ custom rules \+ agent autonomy**: the AI wasn’t just pulling from training memory; it was using the exact text of provided sources to create a summary, much like a human researcher would.

**How to do this in practice:** Drop in your source texts, then either mention them in your prompt or even incorporate them into AGENTS.md (for instance, “We have the following sources available: \[list of files\]. Use them as primary references.”). Cursor 2.0 often auto-discovers nearby files, so the agent may reference them without extra work, but explicitly calling `@references/...` guarantees inclusion when it matters. The legacy “Add Context” panel was pared back in 2.0; rely on prompts plus AGENTS.md instructions, using @filename syntax as your lightweight override when you need to spotlight a document[\[16\]](https://forum.cursor.com/t/cursor-prompt-engineering-best-practices/1592#:~:text=for%20example%2C%20let%E2%80%99s%20say%20that,been%20doing%20something%20like%20this)[\[38\]](https://cursor.com/changelog/2-0).

Remember that if your documents are very large (tens of thousands of words), you might need to give the agent targeted sections (or use Claude’s large context to your advantage). You can ask the agent to summarize a long document first, then work from the summary in subsequent steps, to avoid hitting context limits.

### 2\. **Using Cursor’s Web Search / Browsing Tools**

If you don’t already have the sources, you can instruct the agent to find them. For example: “Search for the latest IPCC report on ocean temperatures. Summarize the key points and provide the reference.” The agent, if it has access to a browser tool, will perform a web search and retrieve information. It might then produce an answer like a summary paragraph with a citation or footnote linking to the IPCC report (assuming it finds it).

It’s wise to guide the agent on what type of sources to seek. You can say “search for peer-reviewed studies via Google Scholar” or “find data on PubMed regarding X”. By being specific (“via Google Scholar” or “on PubMed”), you trigger the agent to use those channels if possible. For instance, an agent augmented with the right tools can directly query PubMed for literature[\[17\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=,Statistical%20analysis%20and%20hypothesis%20testing). In practice you do this by launching an Anthropic client (Claude Desktop or the Claude Code extension inside Cursor) with Claude Skills enabled—those prebuilt workflows cover PubMed, arXiv, and other scholarly sources, which is far more reliable than generic web search.

While the agent can do a lot, **don’t fully rely on it to pick only correct sources**. After it provides results, double-check the sources it cited: are they real? Are the quotes accurate? Cursor’s agent tries to follow your rules, but it can still make mistakes (e.g., citing a source that exists but not actually supporting the claim made). Part of your research workflow should involve verifying at least the key references. A useful pattern is to ask the agent for the *source URLs or DOIs* along with the summary, so you can click and inspect the original.

### 3\. **Leveraging Claude’s Scientific Skills (MCP Plugins)**

Cursor ships with its own MCP client: open `Settings > Context > MCP` to register servers once, and every agent—Composer, GPT-4/5, or Claude—can invoke those tools during a run.

If you invoke Anthropic’s Claude models—either through Cursor’s API settings or by launching the Claude Code extension inside Cursor—you can tap into an **advanced plugin system called Claude Skills**, delivered via Anthropic’s Model Context Protocol (MCP). MCP is an open standard, so the same skill packs run in Claude Desktop, Claude Code CLI, OpenAI Codex, and Cursor once each client is configured.[\[41\]](https://docs.anthropic.com/claude/docs/claude-code-overview)[\[42\]](https://modelcontextprotocol.io/docs/getting-started/intro) In simple terms, these are like plugins or mini-experts that Claude can invoke when needed. K-Dense AI has released a whole collection of **scientific skills** for Claude[[18]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=A%20comprehensive%20collection%20of%20ready,Dense%20team)[[19]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=Category%20Count%20Description%20Scientific%20Databases,initialization%20and%20resource%20detection%20utilities). These skills enable Claude to do things like: query scientific databases (PubMed, PubChem, UniProt, etc.), use specialized libraries (for bioinformatics, chemistry, machine learning, visualization, and more), and follow multi-step scientific workflows out-of-the-box[[20]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=These%20skills%20enable%20Claude%20to,databases%20across%20multiple%20scientific%20domains)[[21]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=Scientific%20Databases%2026%20PubMed%2C%20PubChem%2C,Commons%2C%20histolab%2C%20LaminDB%2C%20PathML%2C%20PyLabRobot). In essence, they turn Claude into an “AI Scientist” with a broad toolkit[[20]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=These%20skills%20enable%20Claude%20to,databases%20across%20multiple%20scientific%20domains). Pair them with Claude Code’s core capabilities—agentic codebase search, coordinated multi-file edits with human approval gates, GitHub task delegation, and repeatable Skills workflows—to keep the Anthropic agent competitive with Cursor’s Composer.[\[41\]](https://docs.claude.com/en/docs/claude-code/overview)[\[49\]](https://www.anthropic.com/news/skills)

**Why use skills?** They can drastically reduce the effort of integration. For example, instead of parsing a PDF yourself, there might be a skill to read and summarize PDFs. Instead of manually scraping PubMed, the PubMed skill can fetch abstracts for you. The Claude Scientific Skills repository includes **26 database connectors (from PubMed to clinical trials), 59 scientific Python packages (for stats, bio, chem, etc.), and numerous example workflows**[\[21\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=Scientific%20Databases%2026%20PubMed%2C%20PubChem%2C,Commons%2C%20histolab%2C%20LaminDB%2C%20PathML%2C%20PyLabRobot). With these, you could ask for fairly high-level tasks and Claude will know which tool/skill to apply.

To use these in Cursor, the typical process is: install the skill plugins and possibly connect to an MCP server that provides them. According to the documentation, you can add the K-Dense scientific skills via a one-click hosted MCP server for Cursor[\[22\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=Cursor). This likely involves installing a Cursor extension or running a command (the snippet suggests a link on cursor.com for installation). Once installed, you might select which categories of skills to enable (e.g. “scientific-databases”, “scientific-packages”, etc.)[\[23\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=%2Fplugin%20marketplace%20add%20K). After that, the agent will automatically have those capabilities.

Crucially, one of the skills provided is called **“scientific-context-initialization”**, which *updates your AGENTS.md file* to remind Claude to use the available skills[\[24\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=automatically%20creates%2Fupdates%20an%20,and%20best%20practices%20from%20the). This is a clever step: it ensures that the AI agent always checks if a skill can handle a task **before** it tries to do it itself[\[24\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=automatically%20creates%2Fupdates%20an%20,and%20best%20practices%20from%20the). For example, if you ask “plot this data,” a naive AI might try to generate a plot in ASCII or describe it, but with the right skill installed and AGENTS.md note, Claude will realize it can use a plotting library skill to actually produce a graph. In our research context, that means if you ask for a literature review, Claude sees in AGENTS.md that it should leverage the literature search skill rather than guessing from memory. This dramatically improves the quality and reliability of the output, since the skill will fetch real data or follow a proven method[\[25\]](https://www.reddit.com/r/ClaudeAI/comments/1oj109n/claude_skills_are_just_cursorrules_change_my_mind/#:~:text=Cursor%20rules%20are%20like%20CLAUDE,Claude%20file%2C%20but%20the%20engineering)[\[26\]](https://www.reddit.com/r/ClaudeAI/comments/1oj109n/claude_skills_are_just_cursorrules_change_my_mind/#:~:text=No%20not%20quite,have%20to%20explicitly%20activate%20it).

After installing skills, your prompting can be more high-level. For example, you could simply say: “Find recent studies about coral bleaching in the Great Barrier Reef, summarize their findings, and provide any data on reef health metrics. Use available skills as needed.” Because you’ve instructed “use available skills” (and your AGENTS.md or the context-init skill reiterates this), Claude might automatically invoke the PubMed skill to find papers, use a summarization skill on each, and even use a data visualization skill if you asked for plotting some data. In the output, you would get a nicely structured summary with references. Indeed, the skills repository documentation shows complex multi-step example prompts where Claude does everything from querying databases to generating visualizations to writing a PDF report[\[27\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=End)[\[28\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=,Keep%20the%20output%20organized) – all within a single prompt, thanks to the skills and the instruction to always use them when possible.

To set this up in Cursor, follow the steps in the Claude Skills docs: for Cursor specifically, add their hosted MCP (this connects Cursor’s Claude agent to the skill server)[\[29\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=For%20Cursor%20users%2C%20we%20now,click%20installation). Ensure your AGENTS.md (or project config) mentions any special access keys or endpoints if needed (some skills might require API keys for certain services – the skill docs will say so). For example, if you installed a skill to query a private database, you’d note the credentials or limitations in AGENTS.md as a reminder to the AI[\[30\]](https://devcenter.upsun.com/posts/why-your-readme-matters-more-than-ai-configuration-files/#:~:text=%2A%20Agent,connected%20services). Generally though, most public scientific data sources (PubMed, arXiv, etc.) don’t require API keys for basic usage, so you may not need special config for those.

**Note:** Access to Claude Skills may require Claude Max (for API usage) or an active Claude Code subscription if you rely on Anthropic’s extension. Ensure your setup is compatible—the skills expect Claude 3.0+ as the backend. If you are using OpenAI models only, you won’t have this plugin system available directly, so you’d rely on manual context or external tools.

### 4\. **Verifying and Curating Sources**

No matter how the agent obtains information (provided files, web search, or skills), maintain a critical eye. After the agent composes a section of the report with citations, go through each citation: check that it supports the claim, and fix any formatting issues in the reference list. It’s often helpful to maintain a separate bibliography file or section. You could even instruct the agent: “At the end of report.md, list all references in full APA format under a References heading.” The agent, having the citation details (from PDFs or search results), can attempt this. Still, double-check it because AI sometimes mixes up details (like authors or titles).

One strategy is to ask the agent after the draft: *“For each reference you cited as \[1\], \[2\], etc., provide the full reference information.”* This way you can cross-verify those references.

## Writing the Research Report with Cursor’s Agent

With the groundwork laid (AGENTS.md rules in place, sources accessible, proper tools enabled), you can now focus on the actual research and writing process, in partnership with the AI agent.

**Step 1: Outline the Plan** – Start by asking the agent to create a research plan or outline. For example: *“Draft an outline for the report, covering background, methodology, key findings, and conclusions on \[your topic\]. Ensure each section notes which sources will be used.”* Because of your earlier instructions, the agent might say something like, “Section 2 will cite \[1\] and \[2\] for background on climate trends.” This is useful to see early on if it’s picking relevant sources. You can iteratively refine the outline with the agent until you’re satisfied that it’s comprehensive.

**Step 2: Let the Agent Gather Information** – Go section by section (if the task is large). You can say: “Fill in the Background section by summarizing information from relevant sources (e.g., any overview articles on this topic).” The agent will then possibly do a quick search or use the provided references to compose a few paragraphs. Because AGENTS.md said “2 sources per claim,” you should see citations appearing like “\[1\]\[2\]” in the draft text for that section. Encourage this by prompting: *“Remember to cite at least two studies in this section.”*

If the agent comes back with “\[1\], \[2\]” but not actual reference names, don’t worry – it’s presumably using placeholders or will list them later. You can follow up: “Provide the details for \[1\] and \[2\].” Alternatively, instruct it upfront: “Use inline citations with the authors and year (e.g., (Smith et al., 2023)) for clarity.” Tailor this to the style you want. The key is the agent should consistently cite evidence rather than make unsupported statements.

**Step 3: Review and Edit in Stages** – After the agent writes a chunk, review it. Check if the citations seem plausible and correct. It’s much easier to correct as you go than to fix an entire document at the end. If something looks fishy (e.g., it cited a “Smith 2023” for a fact you suspect is wrong), ask the agent: “Can you verify the claim about X? Did Smith 2023 actually say that?” The agent might double-check itself and adjust. This kind of self-questioning can be done in a separate agent instance or the same chat. Since Cursor supports multiple agents, you could even spin up a secondary agent whose job is to “act as a fact-checker” on the first agent’s output. (This is advanced, but illustrates the flexibility: one agent writes the report, another agent is given the report.md to critique or find unsupported claims.)

**Step 4: Iterate with Thinking Mode if Needed** – For sections that require complex reasoning or analysis (perhaps synthesizing conflicting study results, or performing a calculation from data), use Claude’s thinking mode or ask the agent to “show its reasoning”. You might prompt: “List the steps you will take to analyze this data (you can think step by step).” The agent may then output a plan (thanks to chain-of-thought prompting). You can confirm the plan, then say “Alright, proceed with executing these steps.” This guiding process ensures the agent doesn’t skip important parts. If the agent is coding something (e.g., running a quick statistical test via the Python tool), review the code it writes as well – ensure it matches your intent.

**Step 5: Finalizing the Report** – Once all sections are drafted and reviewed, have the agent read through the entire report for coherence. A great feature of AI is that it can adjust tone or fix flow easily. You can say: “Now read the entire report.md and suggest any improvements or ensure the tone is consistent and the text is clear for a semi-technical reader.” The agent will use the context (the file content) to make suggestions or even directly provide an edited version. Since the agent works within your file system, it might directly apply the edits to the file or show a diff – you can accept changes that you like.

**Use the Agent Review pane before publishing.** Cursor’s 2.0 review view aggregates every file touched in the last run so you can inspect a unified diff, leave inline comments, or trigger follow-up commands without hunting through the project.[38] Treat it like a preflight checklist: confirm citations, headings, and calculations before you sign off.

Finally, ask the agent to double-check the formatting of citations and references section. Since we preserved citations in the format needed (e.g., numbered or author-year), make sure they’re intact (the agent might sometimes accidentally alter the labels – fix any as needed).

## Best Practices and Tips

* **Context Management:** Always be mindful of what information the agent currently has. If you open multiple files or have a long session, remember that models have context size limits. Don’t hesitate to remind the agent of critical info from earlier if the conversation is long (or just point it to the file where that info resides, e.g. “As described in @project\_overview.md, our focus is X”). Cursor’s agents do a decent job of maintaining context, especially with the reranker pulling relevant bits, but it’s still good practice to explicitly reference key points[\[12\]](https://forum.cursor.com/t/cursor-prompt-engineering-best-practices/1592#:~:text=success%20with%20cursor%20is%20all,about%20context%20management)[\[13\]](https://forum.cursor.com/t/cursor-prompt-engineering-best-practices/1592#:~:text=also%20,cursorrules).  
* **Use AGENTS.md and Other Rule Files Dynamically:** You can update AGENTS.md or create additional rule files as you learn what the AI needs. If you notice it keeps making a certain mistake or forgetting something, add a rule. For instance, “Always double-check that a cited paper’s conclusions are accurately represented.” One clever technique used by power-users is to have the AI **help build its own knowledge base**. For example, if you have a lengthy domain-specific document (like a background primer), you can ask the agent to summarize the key points and add them to AGENTS.md[\[13\]](https://forum.cursor.com/t/cursor-prompt-engineering-best-practices/1592#:~:text=also%20,cursorrules). This way, the next time it won’t forget those points. It’s like teaching the AI and then solidifying that teaching in the rules file.  
* **AGENTS.md vs README:** Remember that AGENTS.md is for AI-specific guidance, not general project info[\[30\]](https://devcenter.upsun.com/posts/why-your-readme-matters-more-than-ai-configuration-files/#:~:text=%2A%20Agent,connected%20services). Don’t overload it with every fact of your topic (that’s what source documents are for). Focus on instructions, project constraints, and tool availability in AGENTS.md. If your project also has a human-readable README with domain context, the agent will likely read that too. Actually, the best scenario is your README.md already has a great summary of the research problem – the agent can leverage it. The Upsun developer blog noted that often teams found their AGENTS.md ended up containing what should have been in the README all along[\[31\]](https://devcenter.upsun.com/posts/why-your-readme-matters-more-than-ai-configuration-files/#:~:text=The%20real%20issue%3A%20your%20README,was%20always%20inadequate). So consider improving your README (for humans) in parallel; the agent benefits from both.  
* **Parallel Agents for Complex Projects:** You might experiment with using multiple agents if a single session becomes unwieldy. For example, one agent could focus on gathering a *bibliography file* (just compiling a list of relevant references with short annotations), while another focuses on writing the narrative. You can then have the writer agent use the bibliography prepared by the other. Since Cursor can have agents work in separate branches or worktrees, you could later merge their contributions. This is an advanced technique and may require manual coordination, but showcases how far you can push the agentic paradigm (some developers even set up an “admin agent” to delegate tasks to “worker agents” in parallel[\[32\]](https://www.reddit.com/r/cursor/comments/1klrq64/agent_mcp_the_multiagent_framework_that_changed/#:~:text=The%20framework%20uses%20a%20hierarchical,model)[\[33\]](https://www.reddit.com/r/cursor/comments/1klrq64/agent_mcp_the_multiagent_framework_that_changed/#:~:text=1,tasks%2C%20maintains%20the%20big%20picture) – a concept beyond our scope here, but worth noting as the future of AI assistants).  
* **Human Oversight and Editing:** While Cursor’s agent can automate a lot, think of it truly as an assistant, not an oracle. You are the chief researcher. Use the AI to do the heavy lifting of digging through texts and drafting content, but always apply your expertise to verify and refine. In academic and scientific work, **accuracy is paramount** – an AI might save you time in drafting, but any claims in the final output are *your responsibility*. So treat the AI’s work as a first draft that you will critically review. This includes double-checking all mathematical calculations, verifying that citations actually support the statements, and ensuring no plagiarism or improper use of source text (the agent should paraphrase or quote with attribution; confirm it hasn’t copied large verbatim sections without quotes).  
* **Dealing with Hallucinations:** Despite all precautions, AI can sometimes output a reference that looks real but isn’t (a hallucinated citation), or a factual statement that’s slightly off. If you spot something odd, ask the agent directly: “Provide the source for that detail” or “Is that reference real? I can’t find it.” If it admits an error or can’t produce the source, remove or correct that part. It’s a good practice to cross-search any references through your own browser (e.g., search the title). Using the agent to cross-verify itself, as mentioned, can also help: e.g., a second pass where the agent has to verify each claim against sources can catch issues.  
* **Continuous Learning:** As you use Cursor for research, you’ll discover what works best for your style. Pay attention to the agent’s outputs and adjust your prompts and rules accordingly. If the agent’s answers are too shallow, you might need to enable thinking mode more often or ask for more step-by-step reasoning. If it’s too verbose or going off-track, tighten your instructions (maybe limit the scope of what you ask in one go).

## Conclusion

By setting up clear instructions (via AGENTS.md), leveraging Cursor’s file-based context and agent tools, and integrating advanced capabilities like Claude’s scientific skills, you can transform Cursor into a powerful scientific research assistant. It can autonomously fetch data, read papers, and help write reports complete with citations – a process that might otherwise take many hours of manual work. Importantly, this workflow remains **human-in-the-loop**: you guide the questions, provide high-level direction, and verify the results, while the AI handles the tedious parts.

Remember that the true advantage here is not to completely hand off research to an AI, but to **collaborate** with it. Cursor’s agent can surface information and draft text at a speed and breadth that’s hard for a single human to match (scanning dozens of papers in minutes), but you provide the critical thinking, context, and final judgment. Used wisely, this synergy can significantly accelerate literature reviews, technical writing, or any knowledge-intensive task, all within a reproducible environment (your project files and AGENTS.md act as a record of what was done, which is great for transparency).

So, set up your research project in Cursor, define your goals and rules, and let the agent help shoulder the load of scientific discovery – all while you maintain control and insight. With practice, you’ll develop a smooth workflow where you can simply tell Cursor, *“Research topic X and compile findings into report.md with proper citations,”* and watch as the AI diligently follows the playbook to deliver results[\[34\]](https://forum.cursor.com/t/anyone-use-cursor-for-academic-research-writing/57393#:~:text=Sat%20down%20and%20did%20a,sonnet%20%28w%2Fo%20thinking)[\[24\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=automatically%20creates%2Fupdates%20an%20,and%20best%20practices%20from%20the). Happy researching\!

**Sources:** The techniques above are informed by real user experiences and emerging best practices in AI-assisted development. Notably, Cursor’s agent-centric update in 2025 emphasized custom rules (AGENTS.md) for guiding AI behavior[\[6\]](https://agents.md/#:~:text=README,project%20descriptions%2C%20and%20contribution%20guidelines), and the community has experimented with using Cursor for academic writing by supplying domain rules and reference texts[\[14\]](https://forum.cursor.com/t/anyone-use-cursor-for-academic-research-writing/57393#:~:text=Sat%20down%20and%20did%20a,sonnet%20%28w%2Fo%20thinking). Anthropic’s Claude Skills ecosystem—available through Claude Code or direct Claude API access—can turn a coding AI into a multidisciplinary expert[\[21\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=Scientific%20Databases%2026%20PubMed%2C%20PubChem%2C,Commons%2C%20histolab%2C%20LaminDB%2C%20PathML%2C%20PyLabRobot). By combining **behavioral guidance** (project rules) with **functional extensions** (skills via MCP), users can define *how* the AI works and *what* tools it can use[\[7\]](https://www.reddit.com/r/ClaudeAI/comments/1oj109n/claude_skills_are_just_cursorrules_change_my_mind/#:~:text=The%20essence%20is%20that%20with,functional%20capability), leading to more reliable and powerful research assistance. The references below include documentation and examples of these concepts in action:

[\[1\]](https://www.deeplearning.ai/the-batch/cursor-introduces-a-new-model-built-for-agents/#:~:text=Cursor%20released%20Composer%2C%20its%20first,limited%20features%20and%20paid%20plans)[\[6\]](https://agents.md/#:~:text=README,project%20descriptions%2C%20and%20contribution%20guidelines)[\[7\]](https://www.reddit.com/r/ClaudeAI/comments/1oj109n/claude_skills_are_just_cursorrules_change_my_mind/#:~:text=The%20essence%20is%20that%20with,functional%20capability)[\[21\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=Scientific%20Databases%2026%20PubMed%2C%20PubChem%2C,Commons%2C%20histolab%2C%20LaminDB%2C%20PathML%2C%20PyLabRobot)[\[24\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=automatically%20creates%2Fupdates%20an%20,and%20best%20practices%20from%20the)[\[14\]](https://forum.cursor.com/t/anyone-use-cursor-for-academic-research-writing/57393#:~:text=Sat%20down%20and%20did%20a,sonnet%20%28w%2Fo%20thinking)

---

[\[1\]](https://www.deeplearning.ai/the-batch/cursor-introduces-a-new-model-built-for-agents/#:~:text=Cursor%20released%20Composer%2C%20its%20first,limited%20features%20and%20paid%20plans) [\[5\]](https://www.deeplearning.ai/the-batch/cursor-introduces-a-new-model-built-for-agents/#:~:text=projects,limited%20features%20and%20paid%20plans) Data Points: Cursor introduces a new model built for agents

[https://www.deeplearning.ai/the-batch/cursor-introduces-a-new-model-built-for-agents/](https://www.deeplearning.ai/the-batch/cursor-introduces-a-new-model-built-for-agents/)

[\[2\]](https://davidmelamed.com/2025/08/08/overview-of-advanced-ai-coding-agents-august-2025/#:~:text=,%E2%80%93%20you%20can%20converse%20with) [\[3\]](https://davidmelamed.com/2025/08/08/overview-of-advanced-ai-coding-agents-august-2025/#:~:text=,world%20coding%20tasks) Overview of Advanced AI Coding Agents (August 2025\) | David Melamed

[https://davidmelamed.com/2025/08/08/overview-of-advanced-ai-coding-agents-august-2025/](https://davidmelamed.com/2025/08/08/overview-of-advanced-ai-coding-agents-august-2025/)

[\[4\]](https://forum.cursor.com/t/anyone-use-cursor-for-academic-research-writing/57393#:~:text=I%20get%20the%20sense%20I%E2%80%99ll,experiment%20I%E2%80%99ll%20update%20with%20more) [\[14\]](https://forum.cursor.com/t/anyone-use-cursor-for-academic-research-writing/57393#:~:text=Sat%20down%20and%20did%20a,sonnet%20%28w%2Fo%20thinking) [\[15\]](https://forum.cursor.com/t/anyone-use-cursor-for-academic-research-writing/57393#:~:text=I%20added%20the%20writing%20rule,improved%20the%20draft%20a%20bit) [\[34\]](https://forum.cursor.com/t/anyone-use-cursor-for-academic-research-writing/57393#:~:text=Sat%20down%20and%20did%20a,sonnet%20%28w%2Fo%20thinking) Anyone use Cursor for academic research writing? \- Discussions \- Cursor \- Community Forum

[https://forum.cursor.com/t/anyone-use-cursor-for-academic-research-writing/57393](https://forum.cursor.com/t/anyone-use-cursor-for-academic-research-writing/57393)

[\[6\]](https://agents.md/#:~:text=README,project%20descriptions%2C%20and%20contribution%20guidelines) [\[8\]](https://agents.md/#:~:text=A%20simple%2C%20open%20format%20for,source%20projects) [\[9\]](https://agents.md/#:~:text=Add%20sections%20that%20help%20an,Popular%20choices) [\[10\]](https://agents.md/#:~:text=What%20if%20instructions%20conflict%3F) [\[11\]](https://agents.md/#:~:text=Can%20I%20update%20it%20later%3F) AGENTS.md

[https://agents.md/](https://agents.md/)

[\[7\]](https://www.reddit.com/r/ClaudeAI/comments/1oj109n/claude_skills_are_just_cursorrules_change_my_mind/#:~:text=The%20essence%20is%20that%20with,functional%20capability) [\[25\]](https://www.reddit.com/r/ClaudeAI/comments/1oj109n/claude_skills_are_just_cursorrules_change_my_mind/#:~:text=Cursor%20rules%20are%20like%20CLAUDE,Claude%20file%2C%20but%20the%20engineering) [\[26\]](https://www.reddit.com/r/ClaudeAI/comments/1oj109n/claude_skills_are_just_cursorrules_change_my_mind/#:~:text=No%20not%20quite,have%20to%20explicitly%20activate%20it) Claude Skills are just .cursorrules, change my mind : r/ClaudeAI

[https://www.reddit.com/r/ClaudeAI/comments/1oj109n/claude\_skills\_are\_just\_cursorrules\_change\_my\_mind/](https://www.reddit.com/r/ClaudeAI/comments/1oj109n/claude_skills_are_just_cursorrules_change_my_mind/)

[\[12\]](https://forum.cursor.com/t/cursor-prompt-engineering-best-practices/1592#:~:text=success%20with%20cursor%20is%20all,about%20context%20management) [\[13\]](https://forum.cursor.com/t/cursor-prompt-engineering-best-practices/1592#:~:text=also%20,cursorrules) [\[16\]](https://forum.cursor.com/t/cursor-prompt-engineering-best-practices/1592#:~:text=for%20example%2C%20let%E2%80%99s%20say%20that,been%20doing%20something%20like%20this) Cursor prompt engineering best practices \- Discussions \- Cursor \- Community Forum

[https://forum.cursor.com/t/cursor-prompt-engineering-best-practices/1592](https://forum.cursor.com/t/cursor-prompt-engineering-best-practices/1592)

[\[17\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=,Statistical%20analysis%20and%20hypothesis%20testing) [\[18\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=A%20comprehensive%20collection%20of%20ready,Dense%20team) [\[19\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=Category%20Count%20Description%20Scientific%20Databases,initialization%20and%20resource%20detection%20utilities) [\[20\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=These%20skills%20enable%20Claude%20to,databases%20across%20multiple%20scientific%20domains) [\[21\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=Scientific%20Databases%2026%20PubMed%2C%20PubChem%2C,Commons%2C%20histolab%2C%20LaminDB%2C%20PathML%2C%20PyLabRobot) [\[22\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=Cursor) [\[23\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=%2Fplugin%20marketplace%20add%20K) [\[24\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=automatically%20creates%2Fupdates%20an%20,and%20best%20practices%20from%20the) [\[27\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=End) [\[28\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=,Keep%20the%20output%20organized) [\[29\]](https://github.com/K-Dense-AI/claude-scientific-skills#:~:text=For%20Cursor%20users%2C%20we%20now,click%20installation) GitHub \- K-Dense-AI/claude-scientific-skills: A set of ready to use scientific skills for Claude

[https://github.com/K-Dense-AI/claude-scientific-skills](https://github.com/K-Dense-AI/claude-scientific-skills)

[\[30\]](https://devcenter.upsun.com/posts/why-your-readme-matters-more-than-ai-configuration-files/#:~:text=%2A%20Agent,connected%20services) [\[31\]](https://devcenter.upsun.com/posts/why-your-readme-matters-more-than-ai-configuration-files/#:~:text=The%20real%20issue%3A%20your%20README,was%20always%20inadequate) AGENTS.md: Why your README matters more than AI configuration files – Upsun Developer Center

[https://devcenter.upsun.com/posts/why-your-readme-matters-more-than-ai-configuration-files/](https://devcenter.upsun.com/posts/why-your-readme-matters-more-than-ai-configuration-files/)

[\[32\]](https://www.reddit.com/r/cursor/comments/1klrq64/agent_mcp_the_multiagent_framework_that_changed/#:~:text=The%20framework%20uses%20a%20hierarchical,model) [\[33\]](https://www.reddit.com/r/cursor/comments/1klrq64/agent_mcp_the_multiagent_framework_that_changed/#:~:text=1,tasks%2C%20maintains%20the%20big%20picture) Agent MCP: The Multi-Agent Framework That Changed How I Build Software : r/cursor

[https://www.reddit.com/r/cursor/comments/1klrq64/agent\_mcp\_the\_multiagent\_framework\_that\_changed/](https://www.reddit.com/r/cursor/comments/1klrq64/agent_mcp_the_multiagent_framework_that_changed/)

[\[35\]](https://formulae.brew.sh/cask/codex?utm_source=chatgpt.com) Codex — Homebrew Cask entry. Homebrew Formulae.

[\[36\]](https://developers.openai.com/codex/cli/?utm_source=chatgpt.com) Codex CLI — official installation guide. OpenAI Developers.

[\[37\]](https://openai.com/codex/) Codex | OpenAI — product overview and surface support.

[\[38\]](https://cursor.com/changelog/2-0) New coding model and agent interface · Cursor changelog (2025).

[\[39\]](https://cursor.com/blog/2-0) Introducing Cursor 2.0 and Composer · Cursor blog (2025).

[\[40\]](https://docs.anthropic.com/claude/docs/models-overview) Models overview · Claude Docs. Anthropic (2025).

[\[41\]](https://docs.anthropic.com/claude/docs/claude-code-overview) Claude Code overview · Anthropic Docs (2025).

[\[42\]](https://modelcontextprotocol.io/docs/getting-started/intro) Model Context Protocol – Getting started (2024).

[\[43\]](https://docs.claude.com/en/docs/claude-code/claude-code-on-the-web#best-practices) Claude Code on the web – best practices (2025).

[\[44\]](https://cursor.com/docs/team/commands) Team Commands – organization-wide rules. Cursor Docs (2025).
[\[45\]](https://developers.openai.com/codex/cloud) Codex cloud – managed sandboxes and parallel tasks. OpenAI Developers (2025).
[\[46\]](https://developers.openai.com/codex/ide) Codex IDE extension surface support. OpenAI Developers (2025).
[\[47\]](https://developers.openai.com/codex/sdk) Codex Agents SDK (TypeScript). OpenAI Developers (2025).
[\[48\]](https://github.com/openai/codex/blob/main/docs/config.md#mcp_servers) Codex configuration – MCP servers. OpenAI Codex GitHub (2025).
[\[49\]](https://www.anthropic.com/news/skills) Introducing Claude Skills. Anthropic News (2025).
